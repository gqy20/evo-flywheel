# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

**Evo-Flywheel (è¿›åŒ–ç”Ÿç‰©å­¦å­¦æœ¯é£è½®)** is an AI-driven academic literature analysis system for evolutionary biology researchers. The system automatically collects the latest research papers from multiple sources, intelligently extracts key findings using LLMs, generates daily research reports, and provides semantic search capabilities.

### Current Status

**é‡Œç¨‹ç¢‘ 1-9 (v0.1.0 - v0.1.8) å·²å®Œæˆ** âœ…
- âœ… é¡¹ç›®åˆå§‹åŒ– (uv + ruff + pre-commit)
- âœ… æ•°æ®åº“æ¨¡å‹ (SQLite + Chroma)
- âœ… CRUD æ“ä½œæ¨¡å—
- âœ… å•å…ƒæµ‹è¯•æ¡†æ¶
- âœ… RSS é‡‡é›†å™¨ (feedparser)
- âœ… bioRxiv API é‡‡é›†å™¨
- âœ… æ•°æ®å»é‡æ¨¡å— (DOI + title)
- âœ… é‡‡é›†ç¼–æ’å™¨ (orchestrator)
- âœ… å®šæ—¶è°ƒåº¦å™¨ (APScheduler)
- âœ… CLI å·¥å…· (evo-fetch, evo-init, evo-analyze)
- âœ… LLM åˆ†ææ¨¡å— (GLM-4.7)
- âœ… å‘é‡åµŒå…¥å’Œè¯­ä¹‰æœç´¢
- âœ… FastAPI REST API
- âœ… Streamlit Web ç•Œé¢
- âœ… é£è½®è‡ªåŠ¨åŒ– (4å°æ—¶é—´éš”)
- âœ… æ·±åº¦æŠ¥å‘Šç”Ÿæˆ

**å½“å‰ç‰ˆæœ¬**: v0.1.8

---

## Architecture Overview

### Dual Database Architecture

The system uses two complementary databases:

1. **SQLite** (`evo_flywheel.db`) - Structured data
   - Paper metadata (title, authors, abstract, DOI, journal, source)
   - AI analysis results (importance score, key findings, evolutionary mechanisms)
   - Daily reports, user feedback
   - RSS source configurations

2. **Chroma** (`chroma_db/`) - Vector embeddings
   - Semantic search for paper abstracts
   - Similar paper recommendations
   - Query embeddings for natural language search

Both databases are linked via `paper_id`. Chroma uses the same ID as SQLite for consistency.

### Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| Backend | FastAPI | REST API with auto-documentation |
| Frontend | Streamlit | Quick web interface without frontend expertise |
| Structured Data | SQLite | Zero-config single-file database |
| Vector Data | Chroma | Embedded vector database (PersistentClient) |
| Embeddings | è¿œç¨‹ API | ä½¿ç”¨è¿œç¨‹ embedding æœåŠ¡ (éæœ¬åœ°æ¨¡å‹) |
| LLM | æ™ºè°± GLM-4.7 | Paper analysis (Â¥0.5/1M input, Â¥2/1M output) |
| Scheduler | APScheduler | Daily automated tasks |
| RSS Parsing | feedparser | RSS feed parsing |
| Linting | ruff | Fast Python linter & formatter |
| Package Manager | uv | Fast Python package installer |
| Testing | pytest | Unit testing framework |

### Project Structure (src layout)

```
evo-flywheel/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ evo_flywheel/         # ä¸»åŒ…ç›®å½•
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py         # é…ç½®ç®¡ç† (pydantic-settings)
â”‚       â”œâ”€â”€ logging.py        # æ—¥å¿—é…ç½®
â”‚       â”œâ”€â”€ api/              # FastAPI endpoints âœ…
â”‚       â”‚   â”œâ”€â”€ deps.py       # ä¾èµ–æ³¨å…¥
â”‚       â”‚   â”œâ”€â”€ schemas.py    # Pydantic æ¨¡å‹
â”‚       â”‚   â”œâ”€â”€ main.py       # API å…¥å£
â”‚       â”‚   â””â”€â”€ v1/           # API v1 è·¯ç”±
â”‚       â”œâ”€â”€ db/               # SQLite models and operations âœ…
â”‚       â”‚   â”œâ”€â”€ models.py      # SQLAlchemy models
â”‚       â”‚   â”œâ”€â”€ crud.py        # CRUD operations
â”‚       â”‚   â””â”€â”€ init.py        # æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
â”‚       â”œâ”€â”€ vector/           # Chroma integration âœ…
â”‚       â”‚   â”œâ”€â”€ client.py      # Chroma PersistentClient
â”‚       â”‚   â”œâ”€â”€ embeddings.py  # Embedding æœåŠ¡
â”‚       â”‚   â”œâ”€â”€ storage.py     # å‘é‡å­˜å‚¨
â”‚       â”‚   â”œâ”€â”€ search.py      # è¯­ä¹‰æœç´¢
â”‚       â”‚   â””â”€â”€ hybrid.py      # æ··åˆæœç´¢
â”‚       â”œâ”€â”€ collectors/       # RSS/API data collection âœ…
â”‚       â”‚   â”œâ”€â”€ rss.py         # RSS feed parser
â”‚       â”‚   â”œâ”€â”€ biorxiv.py     # bioRxiv API client
â”‚       â”‚   â”œâ”€â”€ dedup.py       # Deduplication logic
â”‚       â”‚   â””â”€â”€ orchestrator.py # Multi-source coordinator
â”‚       â”œâ”€â”€ scheduler/        # APScheduler tasks âœ…
â”‚       â”‚   â”œâ”€â”€ jobs.py        # Daily collection jobs
â”‚       â”‚   â””â”€â”€ analysis.py    # Analysis scheduling
â”‚       â”œâ”€â”€ analyzers/        # LLM paper analysis âœ…
â”‚       â”‚   â”œâ”€â”€ prompts.py     # Prompt æ¨¡æ¿
â”‚       â”‚   â”œâ”€â”€ llm.py         # GLM-4.7 å°è£…
â”‚       â”‚   â””â”€â”€ batch.py       # æ‰¹é‡å¤„ç†
â”‚       â”œâ”€â”€ reporters/        # Daily report generation âœ…
â”‚       â”‚   â””â”€â”€ generator.py   # æŠ¥å‘Šç”Ÿæˆå™¨
â”‚       â””â”€â”€ web/              # Streamlit UI âœ…
â”‚           â”œâ”€â”€ app.py         # Streamlit åº”ç”¨å…¥å£
â”‚           â”œâ”€â”€ api_client.py  # API å®¢æˆ·ç«¯
â”‚           â””â”€â”€ views/         # é¡µé¢è§†å›¾
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py           # pytest fixtures âœ…
â”‚   â”œâ”€â”€ unit/                 # å•å…ƒæµ‹è¯• âœ…
â”‚   â”‚   â”œâ”€â”€ test_config.py
â”‚   â”‚   â”œâ”€â”€ test_db_models.py
â”‚   â”‚   â”œâ”€â”€ test_db_crud.py
â”‚   â”‚   â””â”€â”€ test_vector_client.py
â”‚   â””â”€â”€ integration/          # é›†æˆæµ‹è¯• (å¾…å¼€å‘)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ sources.yaml          # RSS source configurations âœ…
â”œâ”€â”€ data/                     # Generated data files
â”œâ”€â”€ reports/                  # Daily markdown reports
â”œâ”€â”€ chroma_db/                # Vector database storage
â”œâ”€â”€ pyproject.toml            # é¡¹ç›®é…ç½® (uv) âœ…
â”œâ”€â”€ .env.example              # ç¯å¢ƒå˜é‡æ¨¡æ¿ âœ…
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml    # Pre-commit hooks âœ…
â”œâ”€â”€ README.md
â””â”€â”€ CLAUDE.md
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ src layout:**
- âœ… é¿å…æµ‹è¯•æ—¶çš„éšå¼å¯¼å…¥é—®é¢˜
- âœ… æ›´æ¸…æ™°çš„åŒ…è¾¹ç•Œ
- âœ… æ›´å®¹æ˜“æ‰“åŒ…å’Œå‘å¸ƒ
- âœ… Python å®˜æ–¹æ¨èç»“æ„

### Development Commands (src layout + ruff + uv)

```bash
# ä½¿ç”¨ uv ç®¡ç†ç¯å¢ƒ
uv venv                          # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (Python 3.13)
source .venv/bin/activate         # æ¿€æ´»ç¯å¢ƒ (Windows: .venv\Scripts\activate)
uv pip install -e ".[dev]"       # å®‰è£…é¡¹ç›®ï¼ˆå¼€å‘æ¨¡å¼ï¼Œå«æ‰€æœ‰ä¾èµ–ï¼‰

# å®‰è£… pre-commit hooks (é¦–æ¬¡è¿è¡Œ)
pre-commit install                # å®‰è£… Git hooks

# åˆå§‹åŒ–æ•°æ®åº“
evo-init                        # ä½¿ç”¨ CLI å·¥å…·
# æˆ–
uv run python -m src.evo_flywheel.db.init

# æ•°æ®é‡‡é›†
evo-fetch                       # æ‰§è¡Œä¸€æ¬¡é‡‡é›† (é»˜è®¤æœ€è¿‘7å¤©)
evo-fetch --schedule            # å¯åŠ¨å®šæ—¶è°ƒåº¦å™¨ (æ¯æ—¥ 9:00)

# è®ºæ–‡åˆ†æ
evo-analyze                     # åˆ†ææœªåˆ†æçš„è®ºæ–‡ (é»˜è®¤ 10 ç¯‡)
evo-analyze --limit 50          # åˆ†æ 50 ç¯‡è®ºæ–‡

# å¯åŠ¨æœåŠ¡å™¨
./start.sh                      # åŒæ—¶å¯åŠ¨ API å’Œ Web ç•Œé¢
# æˆ–åˆ†åˆ«å¯åŠ¨:
uvicorn evo_flywheel.api.main:app --reload  # FastAPI (ç«¯å£ 8000)
streamlit run src/evo_flywheel/web/app.py   # Streamlit (ç«¯å£ 8501)

# ä»£ç æ£€æŸ¥å’Œæ ¼å¼åŒ– (ruff)
ruff check .                    # æ£€æŸ¥ä»£ç 
ruff check . --fix              # æ£€æŸ¥å¹¶è‡ªåŠ¨ä¿®å¤
ruff format .                   # æ ¼å¼åŒ–ä»£ç 

# æ‰‹åŠ¨è¿è¡Œ pre-commit
pre-commit run --all-files      # æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶

# è¿è¡Œæµ‹è¯•
pytest                          # è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/unit/              # åªè¿è¡Œå•å…ƒæµ‹è¯•
pytest tests/api/               # åªè¿è¡Œ API æµ‹è¯•
pytest -v                       # è¯¦ç»†è¾“å‡º
pytest --cov=src/evo_flywheel   # æµ‹è¯•è¦†ç›–ç‡
pytest -k "test_papers"         # è¿è¡ŒåŒ¹é…åç§°çš„æµ‹è¯•
pytest tests/api/test_papers.py::test_get_papers -v  # è¿è¡Œå•ä¸ªæµ‹è¯•
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ uv:**
- âœ… æ¯” pip å¿« 10-100 å€
- âœ… ç»Ÿä¸€çš„ä¾èµ–è§£æ
- âœ… æ›´å¥½çš„é”æ–‡ä»¶æ”¯æŒ
- âœ… ç°ä»£åŒ–çš„ Python å·¥å…·é“¾

**Pre-commit Hooks:**
- è‡ªåŠ¨è¿è¡Œ ruff lint å’Œ format
- æ£€æŸ¥ YAML/TOML è¯­æ³•
- å®‰å…¨æ£€æŸ¥ (bandit)
- è·³è¿‡ hook: `git commit --no-verify`

---

## Data Flow

```
1. Collection (Daily 09:00)
   RSS feeds + bioRxiv API
   â†’ Deduplication (DOI/title)
   â†’ Metadata extraction
   â†’ Store in SQLite

2. Analysis
   New papers from SQLite
   â†’ LLM (GLM-4.7) analysis
   â†’ Extract: taxa, scale, method, findings, mechanism, score
   â†’ Update SQLite

3. Vectorization
   New papers
   â†’ Generate embeddings (è¿œç¨‹ API)
   â†’ Store in Chroma with metadata

4. Search (On-demand)
   User query
   â†’ Query embedding (è¿œç¨‹ API)
   â†’ Chroma similarity search
   â†’ Hybrid: SQLite filters + Chroma ranking

5. Reporting
   Daily aggregation
   â†’ Top papers by score
   â†’ Markdown report template
   â†’ Save to reports/
```

---

## Key Configuration Files

### RSS Sources (`config/sources.yaml`)

The system supports 30+ evolutionary biology journals. See `docs/rss.md` for the complete list. The MVP minimum is 7 sources:

**MVP Configuration:**
- arXiv q-bio.PE (Populations & Evolution)
- bioRxiv API (evolutionary_biology category)
- PLOS Computational Biology
- Methods in Ecology & Evolution
- Molecular Ecology
- Nature
- PLOS Biology

### Environment Variables (`.env`)

```bash
# LLM API (OpenAI å…¼å®¹ï¼Œç”¨äºæ™ºè°±/é€šä¹‰ç­‰)
OPENAI_API_KEY=your-api-key
OPENAI_BASE_URL=https://open.bigmodel.cn/api/paas/v4  # æ™ºè°± GLM

# Embedding API (ç”¨äºè¯­ä¹‰æœç´¢)
EMBEDDING_API_URL=https://api.openai.com/v1
EMBEDDING_API_KEY=your-embedding-api-key
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSION=1536

# Database
DATABASE_URL=sqlite:///data/evo_flywheel.db
CHROMA_PERSIST_DIR=./chroma_db

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/evo_flywheel.log
```

---

## LLM Analysis Schema

The system uses a structured prompt to extract evolutionary biology insights:

```python
ANALYSIS_SCHEMA = {
    "åŸºç¡€ä¿¡æ¯": {
        "ç ”ç©¶ç‰©ç§": "Involved taxa/classification",
        "è¿›åŒ–å°ºåº¦": "Molecular/Individual/Population/Species",
        "ç ”ç©¶æ–¹æ³•": "Phylogenetic/Population Genetic/Experimental/Comparative"
    },
    "æ ¸å¿ƒå†…å®¹": {
        "å…³é”®å‘ç°": "3-5 key findings",
        "è¿›åŒ–æœºåˆ¶": "Natural selection/Genetic drift/Gene flow/Mutation",
        "åˆ›æ–°æ€§": "Theoretical/Methodological/Discovery innovation"
    },
    "ä»·å€¼è¯„ä¼°": {
        "é‡è¦æ€§è¯„åˆ†": "0-100 score",
        "æ¨èç†ç”±": "1-2 sentences explaining value"
    }
}
```

---

## API Endpoints

The FastAPI runs on `http://localhost:8000` with interactive docs at `/api/v1/docs`.

**Key Routes** (all under `/api/v1/`):
- `GET /papers` - List papers with pagination and filters
- `GET /papers/{id}` - Get paper details
- `POST /papers/{id}/analyze` - Analyze a paper with LLM
- `GET /search/semantic` - Semantic search by query
- `POST /search/similar` - Find similar papers
- `GET /search/hybrid` - Hybrid search (semantic + filters)
- `GET /reports/today` - Get today's report
- `POST /collection/fetch` - Trigger data collection
- `POST /analysis/trigger` - Trigger batch analysis
- `POST /embeddings/rebuild` - Rebuild vector index
- `GET /stats/overview` - System statistics

See `docs/api.md` for complete API documentation.

---

## Development Phases

See `docs/ROADMAP.md` for detailed 6-phase development plan (2-3 weeks MVP):

1. **Phase 0**: Project initialization (0.5d) âœ… å®Œæˆ
2. **Phase 1**: Data layer - SQLite + Chroma setup (2d) âœ… å®Œæˆ
3. **Phase 2**: Collection layer - RSS + bioRxiv API (2d) âœ… å®Œæˆ
4. **Phase 3**: Analysis layer - LLM integration (2d) âœ… å®Œæˆ
5. **Phase 4**: Search layer - Embeddings + semantic search (1.5d) âœ… å®Œæˆ
6. **Phase 5**: Presentation layer - Streamlit UI (3d) âœ… å®Œæˆ
7. **Phase 6**: Testing & optimization (2d) ğŸ”„ è¿›è¡Œä¸­

### Completed Milestones

**v0.1.0 - åŸºç¡€è®¾æ–½** âœ…
- é¡¹ç›®åˆå§‹åŒ– (uv + ruff + pre-commit)
- æ•°æ®åº“Schemaè®¾è®¡ (SQLite + Chroma)
- å•å…ƒæµ‹è¯•æ¡†æ¶

**v0.1.1 - æ•°æ®é‡‡é›†å±‚** âœ…
- RSS feed parser with advanced DOI extraction
- bioRxiv API client (avoiding Cloudflare)
- Cross-source deduplication (DOI + title normalization)
- Multi-source orchestrator with graceful error handling
- APScheduler with CLI entry points (`evo-fetch`, `evo-init`)

**v0.1.2 - LLM åˆ†æå±‚** âœ…
- LLM paper analysis (GLM-4.7 via OpenAI-compatible API)
- Structured prompts for evolutionary biology insights
- Batch analysis with progress tracking

**v0.1.3 - æœç´¢å±‚** âœ…
- Vector embeddings (remote API)
- Semantic search with Chroma
- Hybrid search (metadata filters + semantic ranking)

**v0.1.4 - Web ç•Œé¢** âœ…
- Streamlit web interface with multiple views
- Home page with statistics and recommendations
- Paper list, search, and detail views

**v0.1.5 - æµ‹è¯•ä¼˜åŒ–ä¸éƒ¨ç½²** âœ…
- Enhanced testing coverage
- Performance optimization
- Bug fixes and refinements

**v0.1.6 - FastAPI åç«¯** âœ…
- REST API with comprehensive endpoints
- OpenAPI documentation (Swagger/ReDoc)
- Unified error handling

**v0.1.7 - Web UI åç«¯é›†æˆ** âœ…
- APIClient for backend communication
- Paper detail page with feedback
- Integrated search and reporting

**v0.1.8 - é£è½®æ§åˆ¶** âœ…
- Automated flywheel (4-hour interval)
- Deep report generation with LLM
- Multiple reports per day support

---

## Performance Targets

| Operation | Target |
|-----------|--------|
| RSS collection | < 2 minutes |
| Single paper analysis | < 10 seconds |
| Batch analysis (30 papers) | < 5 minutes |
| Semantic search | < 1 second |
| Report generation | < 30 seconds |
| UI page load | < 3 seconds |

---

## Important Implementation Details

### Configuration Access Pattern

Always use `get_settings()` to access configuration, never instantiate `Settings` directly:

```python
from evo_flywheel.config import get_settings

settings = get_settings()
api_key = settings.openai_api_key
```

The `database_url` vs `database_path` logic is handled by `settings.effective_database_url` property.

### Database Session Pattern

Use the `get_db()` dependency for FastAPI endpoints. For scripts, use `SessionLocal()` context manager:

```python
from evo_flywheel.db import SessionLocal

with SessionLocal() as db:
    papers = get_papers(db, limit=10)
```

### DOI Extraction and Normalization

The RSS collector uses advanced regex patterns in `collectors/rss.py` to extract DOIs from various formats. DOIs are normalized (lowercased, whitespace stripped) before storage. The deduplication logic in `collectors/dedup.py` checks both DOI and title similarity.

### LLM API Integration

The project uses OpenAI-compatible API (not just OpenAI). Configure via `OPENAI_BASE_URL` for providers like Zhipu (æ™ºè°± GLM). The `analyzers/llm.py` module wraps the OpenAI client with retry logic and structured JSON response parsing.

### Vector Storage Linking

Chroma collection uses the same `paper_id` as SQLite for consistency. When storing embeddings, always include metadata (title, source, taxa) for hybrid search filtering.

### Error Handling in Collectors

Each collector (RSS, bioRxiv API) should handle errors gracefully. The orchestrator continues even if one source fails. Check logs for individual source errors.

---

## Chroma PersistentClient (æ–°ç‰ˆ API)

```python
import chromadb

client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection("evolutionary_papers")
```

**æ³¨æ„**: ä¸å†ä½¿ç”¨æ—§çš„ `Settings(chroma_db_impl="duckdb+parquet")` APIã€‚

### Remote Embedding API

ä½¿ç”¨è¿œç¨‹ embedding æœåŠ¡ï¼ˆéæœ¬åœ°æ¨¡å‹ï¼‰ï¼Œéœ€åœ¨ç¯å¢ƒå˜é‡ä¸­é…ç½®ï¼š
- `EMBEDDING_API_URL`: Embedding API ç«¯ç‚¹
- `EMBEDDING_API_KEY`: API å¯†é’¥
- `EMBEDDING_MODEL`: æ¨¡å‹åç§°

### bioRxiv API vs RSS

Use the **bioRxiv API** instead of RSS:
- RSS is protected by Cloudflare (requires JavaScript rendering)
- API returns structured JSON, supports date range and category filtering
- Endpoint: `https://api.biorxiv.org/details/biorxiv/{start}/{end}?category=evolutionary_biology`

### Hybrid Search Pattern

1. Use SQLite to filter by metadata (taxa, min_score, journal)
2. Use Chroma to rank by semantic similarity
3. Merge and return top-k results

---

## Reference Documentation

- `docs/PRD.md` - Complete product requirements (v1.2)
- `docs/ROADMAP.md` - Development roadmap with task breakdown
- `docs/rss.md` - 30+ evolutionary biology journal RSS sources
